package model

/**
 * From https://bbycroft.net/llm
 * The self-attention layer is perhaps the heart of the Transformer and of GPT.
 * It's the phase where the columns in our input embedding matrix "talk" to each other.
 *
 * The self-attention layer is made up of several heads, and we'll focus on one of them for now.
 */
class CausalSelfAttention {
}